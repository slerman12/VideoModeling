{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_VideoGPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXfZXsNhy08r"
      },
      "source": [
        "# Using VideoGPT\n",
        "This is a notebook demonstrating how to use VideoGPT and any pretrained models, Make sure that it is a GPU instance: **Change Runtime Type -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ponLMda7zBmF"
      },
      "source": [
        "## Installation\n",
        "First, we install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD86SgJ1yvwZ"
      },
      "source": [
        "! pip install git+https://github.com/wilson1yan/VideoGPT.git\n",
        "! pip install scikit-video av"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G1lfDiwycLl"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision.io import read_video, read_video_timestamps\n",
        "\n",
        "from videogpt import download, load_vqvae\n",
        "from videogpt.data import preprocess\n",
        "\n",
        "VIDEOS = {\n",
        "    'breakdancing': '1OZBnG235-J9LgB_qHv-waHZ4tjofiDgj',\n",
        "    'bear': '16nIaqq2vbPh-WMo_7hs9feVSe0jWVXLF',\n",
        "    'jaywalking': '1UxKCVrbyXhvMz_H7dI4w5hjPpRGCAApy',\n",
        "    'cartoon': '1ONcTMSEuGuLYIDbX-KeFqd390vbTIH9d'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_RyF9AU2tzK"
      },
      "source": [
        "## Downloading the Model\n",
        "There are four pretrained models available: `bair_stride4x2x2`, `ucf101_stride4x4x4`, `kinetics_stride4x4x4`, and `kinetics_stride2x4x4`. BAIR was trained on 64 x 64 video, and the rest on 128 x 128. The `stride` component represents the THW downsampling the VQ-VAE performs on the video tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t5fEML30L3f"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "vqvae = load_vqvae('kinetics_stride2x4x4', device=device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QMgNCPo3jQg"
      },
      "source": [
        "## Video Loading and Preprocessing\n",
        "The code below downloads, loads, and preprocesses a given `mp4` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_FYfsIf2kwU"
      },
      "source": [
        "video_name = 'jaywalking'\n",
        "# `resolution` must be divisible by the encoder image stride\n",
        "# `sequence_length` must be divisible by the encoder temporal stride\n",
        "resolution, sequence_length = vqvae.hparams.resolution, 16\n",
        "\n",
        "video_filename = download(VIDEOS[video_name], f'{video_name}.mp4')\n",
        "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
        "video = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
        "video = preprocess(video, resolution, sequence_length).unsqueeze(0).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3R-ZOi3uri"
      },
      "source": [
        "## VQ-VAE Encoding and Decoding\n",
        "Now, we can encode the video through the `encode` function. The `encode` function also has an optional input `including_embeddings` (default `False`) which will also return the embedding versions of the encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywTjc5wi2odm"
      },
      "source": [
        "with torch.no_grad():\n",
        "    encodings = vqvae.encode(video)\n",
        "    video_recon = vqvae.decode(encodings)\n",
        "    video_recon = torch.clamp(video_recon, -0.5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyzbBVX4J-d"
      },
      "source": [
        "## Visualizing Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2t-dwme2qN1"
      },
      "source": [
        "videos = torch.cat((video, video_recon), dim=-1)\n",
        "videos = videos[0].permute(1, 2, 3, 0) # CTHW -> THWC\n",
        "videos = ((videos + 0.5) * 255).cpu().numpy().astype('uint8')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('real (left), reconstruction (right)')\n",
        "plt.axis('off')\n",
        "im = plt.imshow(videos[0, :, :, :])\n",
        "plt.close()\n",
        "\n",
        "def init():\n",
        "    im.set_data(videos[0, :, :, :])\n",
        "\n",
        "def animate(i):\n",
        "    im.set_data(videos[i, :, :, :])\n",
        "    return im\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY65w8ZS4p0a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}